{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def cleanup_text(texts):\n",
    "    cleaned_text = []\n",
    "    for text in texts:\n",
    "        # remove punctuation\n",
    "        text = re.sub('[!#?,.:\";]|-', ' ', text)\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # remove newline\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        cleaned_text.append(text)\n",
    "    return cleaned_text\n",
    "\n",
    "def get_files(path, dev_size, max_document_length, seed, data_type, tokenizer):\n",
    "    # include_lengths = True - This will cause batch.text to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences.\n",
    "    Text = data.Field(preprocessing=cleanup_text, tokenize=tokenizer, batch_first=True, include_lengths=True, fix_length=max_document_length) # fix_length - make the sentences padded in the same lengths for all the batches\n",
    "    Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
    "\n",
    "    # All files:\n",
    "    fields = [('text', Text), ('labels', Label)]\n",
    "\n",
    "    train_data, test_data = data.TabularDataset.splits(\n",
    "        path=path,\n",
    "        train= data_type + '_train.tsv',\n",
    "        test= data_type + '_test.tsv',\n",
    "        format='tsv',\n",
    "        fields=fields,\n",
    "        skip_header=False\n",
    "    )\n",
    "\n",
    "    train_data, valid_data = train_data.split(split_ratio=dev_size, random_state=random.seed(seed))\n",
    "    print(f'Number of training examples: {len(train_data)}')\n",
    "    print(f'Number of validation examples: {len(valid_data)}')\n",
    "    print(f'Number of testing examples: {len(test_data)}')\n",
    "    return train_data, valid_data, test_data, Text, Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    # define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, output_dim, n_layers,\n",
    "                 bidirectional, dropout, pad_index):\n",
    "        # Constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
    "\n",
    "        # lstm layer\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim1,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim1 * 2, hidden_dim2)\n",
    "        self.fc2 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # activation function\n",
    "        self.act = nn.Softmax() #\\ F.log_softmax(outp)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent_len, emb dim]\n",
    "\n",
    "        # packed sequence\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True) # unpad\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # packed_output shape = (batch, seq_len, num_directions * hidden_size)\n",
    "        # hidden shape  = (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        # concat the final forward and backward hidden state\n",
    "        cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        # output, output_lengths = pad_packed_sequence(packed_output)  # pad the sequence to the max length in the batch\n",
    "\n",
    "        rel = self.relu(cat)\n",
    "        dense1 = self.fc1(rel)\n",
    "\n",
    "        drop = self.dropout(dense1)\n",
    "        preds = self.fc2(drop)\n",
    "\n",
    "        # Final activation function\n",
    "        # preds = self.act(preds)\n",
    "        # preds = preds.argmax(dim=1).unsqueeze(0)\n",
    "        return preds\n",
    "    \n",
    "    \n",
    "    #sequence rep, classification - hidden'ı geri dönecek\n",
    "    #sequence1_hidden = sequence_model text1\n",
    "    #sequence2_hidden = sequence_model text2\n",
    "    #son concat stepini dışarıya çıkart 41'den sonrası"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchtext import data\n",
    "\n",
    "def accuracy(probs, target):\n",
    "  winners = probs.argmax(dim=1)\n",
    "  corrects = (winners == target)\n",
    "  accuracy = corrects.sum().float() / float(target.size(0))\n",
    "  return accuracy\n",
    "\n",
    "######################################## Using torchText ######################################\n",
    "\n",
    "def create_iterator(train_data, valid_data, test_data, batch_size, device):\n",
    "    #  BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
    "    # by setting sort_within_batch = True.\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
    "        batch_size = batch_size,\n",
    "        sort_key = lambda x: len(x.text), # Sort the batches by text length size\n",
    "        sort_within_batch = True,\n",
    "        device = device)\n",
    "    return train_iterator, valid_iterator, test_iterator\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        # retrieve text and no. of words\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.labels.squeeze())\n",
    "\n",
    "        acc = accuracy(predictions, batch.labels)\n",
    "\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, batch.labels)\n",
    "\n",
    "            acc = accuracy(predictions, batch.labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train the model\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        # evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        # save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # placing the tensors on the GPU if one is available.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    path = 'TUBITAK_Project'\n",
    "    path_data = os.path.join(path, \"data\")\n",
    "\n",
    "    # parameters\n",
    "    model_type = \"LSTM\"\n",
    "    data_type = \"morph\" # or: \"token\"\n",
    "\n",
    "    char_based = True\n",
    "    if char_based:\n",
    "        tokenizer = lambda s: list(s) # char-based\n",
    "    else:\n",
    "        tokenizer = lambda s: s.split() # word-based\n",
    "\n",
    "\n",
    "    # hyper-parameters:\n",
    "    lr = 1e-4\n",
    "    batch_size = 50\n",
    "    dropout_keep_prob = 0.5\n",
    "    embedding_size = 300\n",
    "    max_document_length = 100  # each sentence has until 100 words\n",
    "    dev_size = 0.8 # split percentage to train\\validation data\n",
    "    max_size = 5000 # maximum vocabulary size\n",
    "    seed = 1\n",
    "    num_classes = 3\n",
    "\n",
    "    # dropout_keep_prob, embedding_size, batch_size, lr, dev_size, vocabulary_size, max_document_length, input_size, hidden_size, output_dim, n_filters, filter_sizes, num_epochs = get_params(model_type)\n",
    "    train_data, valid_data, test_data, Text, Label = get_files(path_data, dev_size, max_document_length, seed, data_type, tokenizer)\n",
    "\n",
    "    # Build_vocab : It will first create a dictionary mapping all the unique words present in the train_data to an\n",
    "    # index and then after it will use word embedding (random, Glove etc.) to map the index to the corresponding word embedding.\n",
    "    Text.build_vocab(train_data, max_size=max_size)\n",
    "    Label.build_vocab(train_data)\n",
    "    vocab_size = len(Text.vocab)\n",
    "\n",
    "    train_iterator, valid_iterator, test_iterator = create_iterator(train_data, valid_data, test_data, batch_size, device)\n",
    "\n",
    "    # loss function\n",
    "    loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if (model_type == \"LSTM\"):\n",
    "\n",
    "        num_hidden_nodes = 93\n",
    "        hidden_dim2 = 128\n",
    "        num_layers = 2  # LSTM layers\n",
    "        bi_directional = True\n",
    "        num_epochs = 7\n",
    "\n",
    "        to_train = True\n",
    "        pad_index = Text.vocab.stoi[Text.pad_token]\n",
    "\n",
    "        # Build the model\n",
    "        lstm_model = LSTM(vocab_size, embedding_size, num_hidden_nodes, hidden_dim2 , num_classes, num_layers,\n",
    "                       bi_directional, dropout_keep_prob, pad_index)\n",
    "\n",
    "        # optimization algorithm\n",
    "        optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "\n",
    "        # train and evaluation\n",
    "        if (to_train):\n",
    "            # train and evaluation\n",
    "            run_train(num_epochs, lstm_model, train_iterator, valid_iterator, optimizer, loss_func, model_type)\n",
    "\n",
    "        # load weights\n",
    "        lstm_model.load_state_dict(torch.load(\"saved_weights_LSTM.pt\"))\n",
    "        # predict\n",
    "        test_loss, test_acc = evaluate(lstm_model, test_iterator, loss_func)\n",
    "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
